{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oczekujemy kolumn: 25\n",
      "Latest history CSV file: results\\training_history_63_20240911-162357.csv\n",
      "Entries in runs_history.csv:\n",
      "    experiment_id        timestamp model_name  epochs  actual_epochs  \\\n",
      "0               0  20240910-111717       DNN1     350             42   \n",
      "1               1  20240910-112135       DNN2     350             12   \n",
      "2               2  20240910-112332       CNN1     350             14   \n",
      "3               3  20240910-112735       CNN2     350             10   \n",
      "4               4  20240910-113315       CNN3     350             10   \n",
      "5               5  20240910-113817       CNN1     350            249   \n",
      "6               6  20240910-134551       CNN1     350             14   \n",
      "7               7  20240910-152746       CNN1     350             15   \n",
      "8               8  20240910-153129       CNN1     350             16   \n",
      "9               9  20240910-153458       CNN1     350             16   \n",
      "10             10  20240910-153815       CNN2     350             10   \n",
      "11             11  20240910-154347       CNN2     350             10   \n",
      "12             12  20240910-154936       CNN2     350             10   \n",
      "13             13  20240910-155438       CNN2     350             10   \n",
      "14             14  20240910-155908       CNN3     350             11   \n",
      "15             15  20240910-160529       CNN3     350             12   \n",
      "16             16  20240910-161200       CNN3     350             12   \n",
      "17             17  20240910-161931       CNN3     350             13   \n",
      "18             18  20240910-162702       CNN1     350             20   \n",
      "19             19  20240910-163054       CNN2     350             11   \n",
      "20             20  20240910-163619       CNN3     350             12   \n",
      "21             21  20240910-164125       CNN1     350             15   \n",
      "22             22  20240910-164425       CNN1     350             14   \n",
      "23             23  20240910-164710       CNN1     350              9   \n",
      "24             24  20240910-173017       CNN1     350             13   \n",
      "25             25  20240910-173223       CNN1     350             71   \n",
      "26             26  20240910-174647       CNN1     350             48   \n",
      "27             27  20240910-175544       CNN1     350             34   \n",
      "28             28  20240910-195110       CNN2     350             49   \n",
      "29             29  20240910-200324       CNN2     350             13   \n",
      "30             30  20240910-200746       CNN3     350             73   \n",
      "31             31  20240910-204545       CNN3     350            181   \n",
      "32             32  20240911-013726       CNN1     350             18   \n",
      "33             33  20240911-014720       CNN2     350             92   \n",
      "34             34  20240911-023308       CNN3     350             50   \n",
      "35             35  20240911-025055       CNN1     350             43   \n",
      "36             36  20240911-030643       CNN2     350             91   \n",
      "37             37  20240911-034450       CNN3     350             84   \n",
      "38             43  20240911-041419       CNN4     350             20   \n",
      "39             44  20240911-042012       CNN5     350             11   \n",
      "40             45  20240911-042302       CNN4     350             20   \n",
      "41             46  20240911-042922       CNN4     350              9   \n",
      "42             47  20240911-043230       CNN4     350             10   \n",
      "43             48  20240911-043603       CNN4     350             13   \n",
      "44             49  20240911-044021       CNN4     350             54   \n",
      "45             50  20240911-045711       CNN5     350             13   \n",
      "46             51  20240911-050045       CNN5     350             16   \n",
      "47             52  20240911-050457       CNN5     350             18   \n",
      "48             53  20240911-050943       CNN5     350             10   \n",
      "49             54  20240911-051231       CNN5     350             21   \n",
      "50             55  20240911-051809       CNN4     350             30   \n",
      "51             56  20240911-053035       CNN5     350             90   \n",
      "52             57  20240911-060137       CNN4     350             19   \n",
      "53             58  20240911-060940       CNN5     350            120   \n",
      "54             59  20240911-065152       CNN4     350             77   \n",
      "55             60  20240911-071513       CNN4     350            339   \n",
      "56             61  20240911-153845       CNN5     350             71   \n",
      "57             62  20240911-155616       CNN5     350            107   \n",
      "58             63  20240911-162357       CNN5     350             45   \n",
      "\n",
      "    batch_size  dropout_rate  learning_rate optimizer  augmentation  ...  \\\n",
      "0           64           0.0        0.00010      adam         False  ...   \n",
      "1           64           0.0        0.00010      adam         False  ...   \n",
      "2           64           0.0        0.00010      adam         False  ...   \n",
      "3           64           0.0        0.00010      adam         False  ...   \n",
      "4           64           0.0        0.00010      adam         False  ...   \n",
      "5           64           0.0        0.00010      adam          True  ...   \n",
      "6           64           0.1        0.00010      adam         False  ...   \n",
      "7           64           0.2        0.00010      adam         False  ...   \n",
      "8           64           0.3        0.00010      adam         False  ...   \n",
      "9           64           0.4        0.00010      adam         False  ...   \n",
      "10          64           0.1        0.00010      adam         False  ...   \n",
      "11          64           0.2        0.00010      adam         False  ...   \n",
      "12          64           0.3        0.00010      adam         False  ...   \n",
      "13          64           0.4        0.00010      adam         False  ...   \n",
      "14          64           0.1        0.00010      adam         False  ...   \n",
      "15          64           0.2        0.00010      adam         False  ...   \n",
      "16          64           0.3        0.00010      adam         False  ...   \n",
      "17          64           0.4        0.00010      adam         False  ...   \n",
      "18         128           0.0        0.00010      adam         False  ...   \n",
      "19         128           0.0        0.00010      adam         False  ...   \n",
      "20         128           0.0        0.00010      adam         False  ...   \n",
      "21          64           0.0        0.10000      adam         False  ...   \n",
      "22          64           0.0        0.01000      adam         False  ...   \n",
      "23          64           0.0        0.00100      adam         False  ...   \n",
      "24          64           0.0        0.00010      adam         False  ...   \n",
      "25          64           0.0        0.00001      adam         False  ...   \n",
      "26          64           0.0        0.00010       sgd         False  ...   \n",
      "27          64           0.0        0.00010  adadelta         False  ...   \n",
      "28          64           0.0        0.00010       sgd         False  ...   \n",
      "29          64           0.0        0.00010  adadelta         False  ...   \n",
      "30          64           0.0        0.00010       sgd         False  ...   \n",
      "31          64           0.0        0.00010  adadelta         False  ...   \n",
      "32          64           0.0        0.00010      adam          True  ...   \n",
      "33          64           0.0        0.00010      adam          True  ...   \n",
      "34          64           0.0        0.00010      adam          True  ...   \n",
      "35          64           0.0        0.00010      adam          True  ...   \n",
      "36          64           0.0        0.00010      adam          True  ...   \n",
      "37          64           0.0        0.00010      adam          True  ...   \n",
      "38         128           0.0        0.00010      adam         False  ...   \n",
      "39         128           0.0        0.00010      adam         False  ...   \n",
      "40          64           0.0        0.10000      adam         False  ...   \n",
      "41          64           0.0        0.01000      adam         False  ...   \n",
      "42          64           0.0        0.00100      adam         False  ...   \n",
      "43          64           0.0        0.00010      adam         False  ...   \n",
      "44          64           0.0        0.00001      adam         False  ...   \n",
      "45          64           0.0        0.10000      adam         False  ...   \n",
      "46          64           0.0        0.01000      adam         False  ...   \n",
      "47          64           0.0        0.00100      adam         False  ...   \n",
      "48          64           0.0        0.00010      adam         False  ...   \n",
      "49          64           0.0        0.00001      adam         False  ...   \n",
      "50          64           0.0        0.00010      adam          True  ...   \n",
      "51          64           0.0        0.00010      adam          True  ...   \n",
      "52          64           0.0        0.00010      adam          True  ...   \n",
      "53          64           0.0        0.00010      adam          True  ...   \n",
      "54          64           0.0        0.00100       sgd         False  ...   \n",
      "55          64           0.0        0.00100  adadelta         False  ...   \n",
      "56          64           0.0        0.00100       sgd         False  ...   \n",
      "57          64           0.0        0.00010  adadelta         False  ...   \n",
      "58          64           0.0        0.00100  adadelta         False  ...   \n",
      "\n",
      "    learning_rate_scheduler  extra_layers  num_residual_blocks  total_seconds  \\\n",
      "0                       NaN           NaN                  NaN     236.817719   \n",
      "1                       NaN           NaN                  NaN      95.523025   \n",
      "2                       NaN           NaN                  NaN     221.574549   \n",
      "3                       NaN           NaN                  NaN     317.539687   \n",
      "4                       NaN           NaN                  NaN     280.404730   \n",
      "5                       NaN           NaN                  NaN    7614.779248   \n",
      "6                       NaN           NaN                  NaN     179.631071   \n",
      "7                       NaN           NaN                  NaN     208.673583   \n",
      "8                       NaN           NaN                  NaN     193.602350   \n",
      "9                       NaN           NaN                  NaN     181.856013   \n",
      "10                      NaN           NaN                  NaN     322.646799   \n",
      "11                      NaN           NaN                  NaN     331.706609   \n",
      "12                      NaN           NaN                  NaN     288.238325   \n",
      "13                      NaN           NaN                  NaN     255.998557   \n",
      "14                      NaN           NaN                  NaN     367.657270   \n",
      "15                      NaN           NaN                  NaN     373.545617   \n",
      "16                      NaN           NaN                  NaN     397.379310   \n",
      "17                      NaN           NaN                  NaN     422.820076   \n",
      "18                      NaN           NaN                  NaN     209.233567   \n",
      "19                      NaN           NaN                  NaN     308.981721   \n",
      "20                      NaN           NaN                  NaN     285.172408   \n",
      "21                      NaN           NaN                  NaN     163.124133   \n",
      "22                      NaN           NaN                  NaN     150.454255   \n",
      "23                      NaN           NaN                  NaN    2561.891283   \n",
      "24                      NaN           NaN                  NaN     112.757877   \n",
      "25                      NaN           NaN                  NaN     822.851513   \n",
      "26                      NaN           NaN                  NaN     519.942680   \n",
      "27                      NaN           NaN                  NaN    6914.664169   \n",
      "28                      NaN           NaN                  NaN     719.717676   \n",
      "29                      NaN           NaN                  NaN     249.642245   \n",
      "30                      NaN           NaN                  NaN    2229.122736   \n",
      "31                      NaN           NaN                  NaN   17456.005162   \n",
      "32                      NaN           NaN                  NaN     575.966871   \n",
      "33                      NaN           NaN                  NaN    2737.769545   \n",
      "34                      NaN           NaN                  NaN    1056.920154   \n",
      "35                      NaN           NaN                  NaN     937.521740   \n",
      "36                      NaN           NaN                  NaN    2276.331929   \n",
      "37                      NaN           NaN                  NaN    1683.545833   \n",
      "38                      NaN           NaN                  NaN     342.781761   \n",
      "39                      NaN           NaN                  NaN     159.318563   \n",
      "40                      NaN           NaN                  NaN     368.368816   \n",
      "41                      NaN           NaN                  NaN     176.035953   \n",
      "42                      NaN           NaN                  NaN     200.045655   \n",
      "43                      NaN           NaN                  NaN     248.627546   \n",
      "44                      NaN           NaN                  NaN     999.323441   \n",
      "45                      NaN           NaN                  NaN     203.037313   \n",
      "46                      NaN           NaN                  NaN     242.058893   \n",
      "47                      NaN           NaN                  NaN     275.522357   \n",
      "48                      NaN           NaN                  NaN     155.564646   \n",
      "49                      NaN           NaN                  NaN     326.811202   \n",
      "50                      NaN           NaN                  NaN     732.362497   \n",
      "51                      NaN           NaN                  NaN    1848.598780   \n",
      "52                      NaN           NaN                  NaN     469.337046   \n",
      "53                      NaN           NaN                  NaN    2517.773372   \n",
      "54                      NaN           NaN                  NaN    1390.570290   \n",
      "55                      NaN           NaN                  NaN   30182.309123   \n",
      "56                      NaN           NaN                  NaN    1040.708353   \n",
      "57                      NaN           NaN                  NaN    1648.707765   \n",
      "58                      NaN           NaN                  NaN     619.453242   \n",
      "\n",
      "    best_train_accuracy  best_val_accuracy  best_train_loss  best_val_loss  \\\n",
      "0              0.625462           0.278846         1.664221       2.805244   \n",
      "1              0.512769           0.193910         1.722133       3.030098   \n",
      "2              0.766538           0.373397         0.847939       2.455961   \n",
      "3              0.817231           0.408654         0.634267       2.289024   \n",
      "4              0.645846           0.298077         1.191648       2.623800   \n",
      "5              0.600787           0.616987         1.383169       1.458651   \n",
      "6              0.727731           0.371795         0.954055       2.477154   \n",
      "7              0.703577           0.394231         1.026481       2.331912   \n",
      "8              0.689846           0.394231         1.066717       2.418529   \n",
      "9              0.641885           0.378205         1.222517       2.455051   \n",
      "10             0.809192           0.399038         0.652907       2.192990   \n",
      "11             0.793538           0.443910         0.699596       2.094260   \n",
      "12             0.779385           0.456731         0.741309       2.169058   \n",
      "13             0.745154           0.440705         0.847297       2.298974   \n",
      "14             0.626269           0.333333         1.210384       2.723919   \n",
      "15             0.604346           0.334936         1.269120       2.713105   \n",
      "16             0.547308           0.306090         1.439647       2.581378   \n",
      "17             0.598000           0.326923         1.290634       2.551839   \n",
      "18             0.745423           0.391026         0.970146       2.321282   \n",
      "19             0.798077           0.408654         0.700744       2.163734   \n",
      "20             0.572538           0.274038         1.437165       2.713583   \n",
      "21             0.040038           0.038462         3.272595       3.267863   \n",
      "22             0.038615           0.038462         3.259813       3.258808   \n",
      "23             0.827192           0.389423         0.610498       2.561452   \n",
      "24             0.770731           0.378205         0.836563       2.379263   \n",
      "25             0.680346           0.341346         1.431741       2.425482   \n",
      "26             0.074423           0.067308         3.257930       3.257981   \n",
      "27             0.040500           0.043269         3.258021       3.258077   \n",
      "28             0.060500           0.046474         3.257930       3.257997   \n",
      "29             0.049423           0.028846         3.258046       3.258082   \n",
      "30             0.057808           0.059295         3.257982       3.257996   \n",
      "31             0.039269           0.052885         3.258022       3.258030   \n",
      "32             0.510834           0.431090         1.671100       2.150273   \n",
      "33             0.891965           0.785256         0.376948       1.008938   \n",
      "34             0.754087           0.538462         0.803451       1.975840   \n",
      "35             0.387916           0.419872         2.092274       2.200055   \n",
      "36             0.845697           0.818910         0.537441       0.717179   \n",
      "37             0.713873           0.549679         0.929889       1.860047   \n",
      "38             0.991846           0.729167         0.032974       1.382705   \n",
      "39             0.769308           0.442308         0.815765       2.379430   \n",
      "40             0.039615           0.038462         3.272910       3.264186   \n",
      "41             0.970269           0.107372         0.108953      10.659410   \n",
      "42             0.993462           0.689103         0.019948       2.070300   \n",
      "43             0.989423           0.703526         0.039625       1.318341   \n",
      "44             0.980500           0.687500         0.078749       1.397517   \n",
      "45             0.039500           0.038462         3.271715       3.268012   \n",
      "46             0.039538           0.038462         3.259609       3.258541   \n",
      "47             0.966654           0.658654         0.106477       2.178931   \n",
      "48             0.807308           0.453526         0.673603       2.442636   \n",
      "49             0.630115           0.331731         1.298268       2.285328   \n",
      "50             0.974630           0.903846         0.084013       0.361823   \n",
      "51             0.914173           0.788462         0.289230       0.889443   \n",
      "52             0.947872           0.918269         0.177063       0.314898   \n",
      "53             0.883560           0.838141         0.388817       0.639947   \n",
      "54             0.975385           0.717949         0.095239       1.206450   \n",
      "55             0.966500           0.637821         0.132895       1.538688   \n",
      "56             0.076462           0.105769         3.257878       3.257890   \n",
      "57             0.125692           0.088141         3.257908       3.257946   \n",
      "58             0.110769           0.097756         3.257882       3.257958   \n",
      "\n",
      "                                   history_csv_file  \\\n",
      "0    results\\training_history_0_20240910-111717.csv   \n",
      "1    results\\training_history_1_20240910-112135.csv   \n",
      "2    results\\training_history_2_20240910-112332.csv   \n",
      "3    results\\training_history_3_20240910-112735.csv   \n",
      "4    results\\training_history_4_20240910-113315.csv   \n",
      "5    results\\training_history_5_20240910-113817.csv   \n",
      "6    results\\training_history_6_20240910-134551.csv   \n",
      "7    results\\training_history_7_20240910-152746.csv   \n",
      "8    results\\training_history_8_20240910-153129.csv   \n",
      "9    results\\training_history_9_20240910-153458.csv   \n",
      "10  results\\training_history_10_20240910-153815.csv   \n",
      "11  results\\training_history_11_20240910-154347.csv   \n",
      "12  results\\training_history_12_20240910-154936.csv   \n",
      "13  results\\training_history_13_20240910-155438.csv   \n",
      "14  results\\training_history_14_20240910-155908.csv   \n",
      "15  results\\training_history_15_20240910-160529.csv   \n",
      "16  results\\training_history_16_20240910-161200.csv   \n",
      "17  results\\training_history_17_20240910-161931.csv   \n",
      "18  results\\training_history_18_20240910-162702.csv   \n",
      "19  results\\training_history_19_20240910-163054.csv   \n",
      "20  results\\training_history_20_20240910-163619.csv   \n",
      "21  results\\training_history_21_20240910-164125.csv   \n",
      "22  results\\training_history_22_20240910-164425.csv   \n",
      "23  results\\training_history_23_20240910-164710.csv   \n",
      "24  results\\training_history_24_20240910-173017.csv   \n",
      "25  results\\training_history_25_20240910-173223.csv   \n",
      "26  results\\training_history_26_20240910-174647.csv   \n",
      "27  results\\training_history_27_20240910-175544.csv   \n",
      "28  results\\training_history_28_20240910-195110.csv   \n",
      "29  results\\training_history_29_20240910-200324.csv   \n",
      "30  results\\training_history_30_20240910-200746.csv   \n",
      "31  results\\training_history_31_20240910-204545.csv   \n",
      "32  results\\training_history_32_20240911-013726.csv   \n",
      "33  results\\training_history_33_20240911-014720.csv   \n",
      "34  results\\training_history_34_20240911-023308.csv   \n",
      "35  results\\training_history_35_20240911-025055.csv   \n",
      "36  results\\training_history_36_20240911-030643.csv   \n",
      "37  results\\training_history_37_20240911-034450.csv   \n",
      "38  results\\training_history_43_20240911-041419.csv   \n",
      "39  results\\training_history_44_20240911-042012.csv   \n",
      "40  results\\training_history_45_20240911-042302.csv   \n",
      "41  results\\training_history_46_20240911-042922.csv   \n",
      "42  results\\training_history_47_20240911-043230.csv   \n",
      "43  results\\training_history_48_20240911-043603.csv   \n",
      "44  results\\training_history_49_20240911-044021.csv   \n",
      "45  results\\training_history_50_20240911-045711.csv   \n",
      "46  results\\training_history_51_20240911-050045.csv   \n",
      "47  results\\training_history_52_20240911-050457.csv   \n",
      "48  results\\training_history_53_20240911-050943.csv   \n",
      "49  results\\training_history_54_20240911-051231.csv   \n",
      "50  results\\training_history_55_20240911-051809.csv   \n",
      "51  results\\training_history_56_20240911-053035.csv   \n",
      "52  results\\training_history_57_20240911-060137.csv   \n",
      "53  results\\training_history_58_20240911-060940.csv   \n",
      "54  results\\training_history_59_20240911-065152.csv   \n",
      "55  results\\training_history_60_20240911-071513.csv   \n",
      "56  results\\training_history_61_20240911-153845.csv   \n",
      "57  results\\training_history_62_20240911-155616.csv   \n",
      "58  results\\training_history_63_20240911-162357.csv   \n",
      "\n",
      "                            tensorboard_log_dir  \n",
      "0    results\\tensorboard_logs_0_20240910-111717  \n",
      "1    results\\tensorboard_logs_1_20240910-112135  \n",
      "2    results\\tensorboard_logs_2_20240910-112332  \n",
      "3    results\\tensorboard_logs_3_20240910-112735  \n",
      "4    results\\tensorboard_logs_4_20240910-113315  \n",
      "5    results\\tensorboard_logs_5_20240910-113817  \n",
      "6   results\\tensorboard_logs_6_20240910-1345517  \n",
      "7    results\\tensorboard_logs_7_20240910-152746  \n",
      "8    results\\tensorboard_logs_8_20240910-153129  \n",
      "9    results\\tensorboard_logs_9_20240910-153458  \n",
      "10  results\\tensorboard_logs_10_20240910-153815  \n",
      "11  results\\tensorboard_logs_11_20240910-154347  \n",
      "12  results\\tensorboard_logs_12_20240910-154936  \n",
      "13  results\\tensorboard_logs_13_20240910-155438  \n",
      "14  results\\tensorboard_logs_14_20240910-155908  \n",
      "15  results\\tensorboard_logs_15_20240910-160529  \n",
      "16  results\\tensorboard_logs_16_20240910-161200  \n",
      "17  results\\tensorboard_logs_17_20240910-161931  \n",
      "18  results\\tensorboard_logs_18_20240910-162702  \n",
      "19  results\\tensorboard_logs_19_20240910-163054  \n",
      "20  results\\tensorboard_logs_20_20240910-163619  \n",
      "21  results\\tensorboard_logs_21_20240910-164125  \n",
      "22  results\\tensorboard_logs_22_20240910-164425  \n",
      "23  results\\tensorboard_logs_23_20240910-164710  \n",
      "24  results\\tensorboard_logs_24_20240910-173017  \n",
      "25  results\\tensorboard_logs_25_20240910-173223  \n",
      "26  results\\tensorboard_logs_26_20240910-174647  \n",
      "27  results\\tensorboard_logs_27_20240910-175544  \n",
      "28  results\\tensorboard_logs_28_20240910-195110  \n",
      "29  results\\tensorboard_logs_29_20240910-200324  \n",
      "30  results\\tensorboard_logs_30_20240910-200746  \n",
      "31  results\\tensorboard_logs_31_20240910-204545  \n",
      "32  results\\tensorboard_logs_32_20240911-013726  \n",
      "33  results\\tensorboard_logs_33_20240911-014720  \n",
      "34  results\\tensorboard_logs_34_20240911-023308  \n",
      "35  results\\tensorboard_logs_35_20240911-025055  \n",
      "36  results\\tensorboard_logs_36_20240911-030643  \n",
      "37  results\\tensorboard_logs_37_20240911-034450  \n",
      "38  results\\tensorboard_logs_43_20240911-041419  \n",
      "39  results\\tensorboard_logs_44_20240911-042012  \n",
      "40  results\\tensorboard_logs_45_20240911-042302  \n",
      "41  results\\tensorboard_logs_46_20240911-042922  \n",
      "42  results\\tensorboard_logs_47_20240911-043230  \n",
      "43  results\\tensorboard_logs_48_20240911-043603  \n",
      "44  results\\tensorboard_logs_49_20240911-044021  \n",
      "45  results\\tensorboard_logs_50_20240911-045711  \n",
      "46  results\\tensorboard_logs_51_20240911-050045  \n",
      "47  results\\tensorboard_logs_52_20240911-050457  \n",
      "48  results\\tensorboard_logs_53_20240911-050943  \n",
      "49  results\\tensorboard_logs_54_20240911-051231  \n",
      "50  results\\tensorboard_logs_55_20240911-051809  \n",
      "51  results\\tensorboard_logs_56_20240911-053035  \n",
      "52  results\\tensorboard_logs_57_20240911-060137  \n",
      "53  results\\tensorboard_logs_58_20240911-060940  \n",
      "54  results\\tensorboard_logs_59_20240911-065152  \n",
      "55  results\\tensorboard_logs_60_20240911-071513  \n",
      "56  results\\tensorboard_logs_61_20240911-153845  \n",
      "57  results\\tensorboard_logs_62_20240911-155616  \n",
      "58  results\\tensorboard_logs_63_20240911-162357  \n",
      "\n",
      "[59 rows x 25 columns]\n",
      "WARNING:tensorflow:From c:\\Users\\zuzan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\zuzan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "`runs_history.csv` not found. Skipping report generation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from models import get_model\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "def debug_csv_file(file_path, expected_columns):\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        num_columns = len(header)\n",
    "        print(f\"Oczekujemy kolumn: {num_columns}\")\n",
    "\n",
    "        for i, row in enumerate(reader, start=2):\n",
    "            if len(row) != num_columns:\n",
    "                print(f\"Problem linia {i}. Oczekjemy {num_columns} kolumn, ale mamy {len(row)}\")\n",
    "                print(f\"Zawartość linii: {row}\")\n",
    "                \n",
    "csv_file_path = 'results/runs_history.csv'\n",
    "debug_csv_file(csv_file_path, expected_columns=18)\n",
    "def find_latest_history_csv(results_dir):\n",
    "    csv_files = [f for f in os.listdir(results_dir) if f.startswith('training_history') and f.endswith('.csv')]\n",
    "    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(results_dir, x)))\n",
    "    return os.path.join(results_dir, latest_file)\n",
    "\n",
    "\n",
    "def generate_plots(history_df, output_dir):\n",
    "    metrics = ['accuracy', 'val_accuracy', 'loss', 'val_loss']\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history_df[metric])\n",
    "        plt.title(f'Model {metric}')\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'] if 'val' in metric else ['Train'], loc='upper left')\n",
    "        plot_path = f'{output_dir}/{metric}_plot.jpg'\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def find_best_results(runs_history_df):\n",
    "    best_run = runs_history_df.loc[runs_history_df['best_val_accuracy'].idxmax()]\n",
    "    return best_run\n",
    "\n",
    "\n",
    "def generate_report(output_dir):\n",
    "    try:\n",
    "        runs_history_df = pd.read_csv('results/runs_history.csv')\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return\n",
    "    history_csv_file = find_latest_history_csv(output_dir)\n",
    "    print(f\"Latest history CSV file: {history_csv_file}\")\n",
    "\n",
    "    if not os.path.exists(history_csv_file):\n",
    "        raise FileNotFoundError(f\"The file {history_csv_file} does not exist.\")\n",
    "\n",
    "    history_df = pd.read_csv(history_csv_file)\n",
    "    generate_plots(history_df, output_dir)\n",
    "\n",
    "    runs_history_df = pd.read_csv('results/runs_history.csv')\n",
    "    print(f\"Entries in runs_history.csv:\\n{runs_history_df}\")\n",
    "\n",
    "    matching_rows = runs_history_df[runs_history_df['history_csv_file'] == history_csv_file]\n",
    "    if not matching_rows.empty:\n",
    "        run_details = matching_rows.iloc[0]\n",
    "    else:\n",
    "        raise ValueError(f\"No matching entry found for {history_csv_file} in runs_history.csv\")\n",
    "\n",
    "    best_run = find_best_results(runs_history_df)\n",
    "\n",
    "    # Extract model architecture\n",
    "    model = get_model(\n",
    "        model_name=run_details['model_name'],\n",
    "        dropout_rate=run_details['dropout_rate'],\n",
    "        learning_rate=run_details['learning_rate'],\n",
    "        optimizer=run_details['optimizer'],\n",
    "        augmentation=run_details['augmentation'],\n",
    "    )\n",
    "    model_summary = []\n",
    "    model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    model_summary_str = \"\\n\".join(model_summary)\n",
    "\n",
    "    report_content = f\"\"\"\n",
    "# Experiment Report\n",
    "\n",
    "## Parameters\n",
    "- **Experiment ID**: {run_details['experiment_id']}\n",
    "- **Timestamp**: {run_details['timestamp']}\n",
    "- **Model Name**: {run_details['model_name']}\n",
    "- **Epochs**: {run_details['epochs']}\n",
    "- **Batch Size**: {run_details['batch_size']}\n",
    "- **Dropout Rate**: {run_details['dropout_rate']}\n",
    "- **Learning Rate**: {run_details['learning_rate']}\n",
    "- **Optimizer**: {run_details['optimizer']}\n",
    "- **Augmentation**: {run_details['augmentation']}\n",
    "  - **Zoom Range**: {run_details['zoom_range']}\n",
    "  - **Rotation Range**: {run_details['rotation_range']}\n",
    "  - **Width Shift Range**: {run_details['width_shift_range']}\n",
    "  - **Height Shift Range**: {run_details['height_shift_range']}\n",
    "  - **Shear Range**: {run_details['shear_range']}\n",
    "- **Total Seconds**: {run_details['total_seconds']}\n",
    "\n",
    "## Model Architecture\n",
    "    \n",
    "    {model_summary_str}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    with open(f'{output_dir}/Report.md', 'w') as report_file:\n",
    "        report_file.write(report_content)\n",
    "    print(\"`runs_history.csv` not found. Skipping report generation.\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "output_dir = 'results'\n",
    "generate_report(output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
